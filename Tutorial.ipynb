{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "Iâ€™m going to explore the practical usages of topic model using the Gensim Library, such as finding structures for unknown datasets, classifying unlabeled data as well as improving accuracy of supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial will introduce you several practical ways of using topic models with **Gensim Library**. In this tutorial I mainly focus on 2 kinds of topic models: *Latent Semantic Indexing (LSI)* and *Latent Dirichlet Allocation (LDA)*. In different cases, topic models can have different functionalities. Throughout this tutorial, you will learn how to use topic models to find structure for unknown datasets, classifying unlabeled data as well as improving the accuracy of supervised learning. Besides, in order to have a better views to the results, this tutorial also introduces some data visualization methods for reviewing topic models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover the following topics in this tutorial:\n",
    "- [Installing the libraries](#Installing-the-libraries)\n",
    "- [Processing text](#Processing-text)\n",
    "- [Training LSI model]\n",
    "- [Training LDA model]\n",
    "- [Finding structures of unknown text]\n",
    "- [Classifying unlabeled data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install the various libraries that we will use.  You can install Gensim, and NLTK using `pip`:\n",
    "\n",
    "    $ pip3 install --upgrade gensim\n",
    "\n",
    "    $ pip3 install -U nltk\n",
    "\n",
    "When doing the examples, you might also need to use scikit-learn:\n",
    "\n",
    "    $ pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import sklearn\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing text\n",
    "Here we use the [20 newsgroups text dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) from sklearn datasets. We also use the same processing method as homework-3. Additionally, we also remove all the stopwords as well as rarewords in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from text_process import process, tokenize, remove_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training and the other one for testing. Since for basic topic models there is no better way to truly evaluate the topics than manually examine the results and see whether they made sense. So here I use both training and testing sets for generating the topic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 67.17970275878906\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "docs_raw = [tokenize(doc) for doc in newsgroups.data]\n",
    "docs = remove_stopwords(docs_raw)\n",
    "k = 1000\n",
    "sample_idxs = random.sample(range(len(docs)), k)  # 1000 samples from docs in order to do quick modeling\n",
    "docs_sample = []\n",
    "docs_raw_sample = []\n",
    "for idx in sample_idxs:\n",
    "    docs_sample.append(docs[idx]) \n",
    "    docs_raw_sample.append(docs_raw[idx])\n",
    "end = time.time()\n",
    "\n",
    "print(\"Processing time: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of docs is: 18846\n",
      "\n",
      "1st from samples of processed document: \n",
      "nutrasweet synthetic sweetener couple thousand time sweeter sugar people concerned chemical body produce degrades nutrasweet thought form formaldehyde known methanol degredation pathway body us eliminate substance real issue whether level methanol formaldehyde produced high enough cause significant damage toxic living cell say consume phenylalanine nothing worry amino acid everyone us small quantity protein synthesis body people disease known missing enzyme necessary degrade compound eliminate body accumulate body high level toxic growing nerve cell therefore major problem young child around age woman pregnant disorder used leading cause brain damage infant easily detected birth one must simply avoid phenylalanine child pregnant\n",
      "\n",
      "1st from processed document:\n",
      "sure bashers pen fan pretty confused lack kind post recent pen massacre devil actually bit puzzled bit relieved however going put end non pittsburghers relief bit praise pen man killing devil worse thought jagr showed much better regular season stats also lot fo fun watch playoff bowman let jagr lot fun next couple game since pen going beat pulp jersey anyway disappointed see islander lose final regular season game pen rule\n"
     ]
    }
   ],
   "source": [
    "# Print out the num of docs and some of the processing results\n",
    "print(\"The number of docs is: \" + str(len(docs)))\n",
    "print()\n",
    "print(\"1st from samples of processed document: \")\n",
    "print(' '.join(docs_sample[0]))\n",
    "print()\n",
    "print(\"1st from processed document:\")\n",
    "print(' '.join(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary_all = gensim.corpora.Dictionary(docs)\n",
    "dictionary_sample = gensim.corpora.Dictionary(docs_sample)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix_all = [dictionary_all.doc2bow(doc) for doc in docs]\n",
    "doc_term_matrix_sample = [dictionary_sample.doc2bow(doc) for doc in docs_sample]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lsi = gensim.models.lsimodel.LsiModel\n",
    "lsimodel = Lsi(doc_term_matrix_sample, num_topics=30, id2word = dictionary_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.382*\"planet\" + 0.270*\"earth\" + 0.225*\"spacecraft\" + 0.223*\"moon\" + 0.206*\"solar\" + 0.184*\"system\" + 0.169*\"surface\" + 0.141*\"sun\" + 0.140*\"venus\" + 0.137*\"atmosphere\"')\n",
      "(1, '-0.597*\"adl\" + -0.305*\"bullock\" + -0.196*\"gerard\" + -0.190*\"group\" + -0.143*\"information\" + -0.133*\"fbi\" + -0.131*\"right\" + -0.126*\"say\" + -0.123*\"francisco\" + -0.122*\"san\"')\n",
      "(2, '0.152*\"m5\" + 0.152*\"mv\" + 0.110*\"mf\" + 0.110*\"m8\" + 0.102*\"mp\" + 0.102*\"md\" + 0.102*\"mj\" + 0.101*\"mh\" + 0.093*\"m4\" + 0.093*\"mw\"')\n",
      "(3, '-0.501*\"kinsey\" + -0.265*\"sex\" + -0.208*\"reisman\" + 0.174*\"adl\" + -0.139*\"child\" + -0.137*\"people\" + -0.119*\"one\" + -0.117*\"sexual\" + -0.111*\"would\" + -0.096*\"boy\"')\n",
      "(4, '0.525*\"space\" + -0.316*\"kinsey\" + 0.243*\"list\" + 0.190*\"post\" + -0.161*\"sex\" + 0.159*\"nasa\" + 0.150*\"shuttle\" + 0.147*\"sci\" + -0.131*\"reisman\" + 0.110*\"one\"')\n",
      "(5, '-0.345*\"space\" + -0.318*\"kinsey\" + 0.198*\"jesus\" + 0.178*\"one\" + -0.157*\"sex\" + 0.155*\"people\" + 0.144*\"would\" + -0.135*\"list\" + -0.132*\"reisman\" + 0.126*\"day\"')\n",
      "(6, '0.470*\"jesus\" + 0.271*\"say\" + 0.186*\"matthew\" + 0.150*\"disciple\" + 0.139*\"mark\" + 0.132*\"mary\" + 0.121*\"heaven\" + 0.120*\"john\" + -0.118*\"people\" + -0.118*\"signal\"')\n",
      "(7, '-0.567*\"de\" + -0.459*\"van\" + -0.323*\"het\" + -0.279*\"een\" + -0.218*\"en\" + -0.160*\"te\" + -0.147*\"utrecht\" + -0.146*\"op\" + -0.132*\"prof\" + -0.117*\"dr\"')\n",
      "(8, '-0.401*\"armenian\" + 0.240*\"signal\" + -0.228*\"people\" + 0.217*\"dce\" + -0.198*\"sy\" + 0.183*\"dtr\" + 0.183*\"dte\" + 0.177*\"data\" + 0.172*\"ct\" + 0.160*\"dsr\"')\n",
      "(9, '-0.426*\"day\" + -0.299*\"cancer\" + 0.256*\"armenian\" + -0.185*\"per\" + -0.157*\"bath\" + -0.142*\"remedy\" + 0.130*\"sy\" + -0.130*\"cure\" + -0.126*\"eat\" + 0.118*\"jesus\"')\n",
      "(10, '0.486*\"armenian\" + 0.237*\"sy\" + 0.194*\"day\" + 0.136*\"signal\" + -0.126*\"god\" + 0.124*\"dce\" + -0.121*\"would\" + 0.121*\"cancer\" + -0.121*\"know\" + -0.120*\"christian\"')\n",
      "(11, '0.741*\"win\" + 0.257*\"main\" + 0.218*\"detail\" + 0.204*\"mydisplay\" + 0.177*\"myhint\" + 0.128*\"event\" + -0.090*\"shuttle\" + -0.081*\"face\" + 0.076*\"line\" + 0.076*\"text\"')\n",
      "(12, '0.268*\"win\" + 0.179*\"shuttle\" + 0.171*\"face\" + 0.151*\"source\" + -0.149*\"list\" + -0.133*\"god\" + 0.133*\"constant\" + -0.132*\"post\" + -0.129*\"christian\" + 0.125*\"mass\"')\n",
      "(13, '-0.254*\"constant\" + -0.238*\"mass\" + -0.235*\"god\" + -0.229*\"christian\" + -0.180*\"km\" + -0.164*\"velocity\" + 0.163*\"right\" + -0.155*\"radius\" + -0.138*\"earth\" + -0.136*\"bible\"')\n",
      "(14, '0.226*\"constant\" + 0.218*\"mass\" + -0.212*\"god\" + -0.204*\"christian\" + 0.164*\"km\" + -0.143*\"win\" + 0.142*\"velocity\" + 0.139*\"bit\" + 0.139*\"radius\" + -0.130*\"shuttle\"')\n",
      "(15, '-0.368*\"right\" + -0.212*\"court\" + -0.205*\"people\" + -0.195*\"boomer\" + -0.173*\"price\" + -0.165*\"cement\" + 0.161*\"bit\" + -0.151*\"property\" + -0.136*\"win\" + 0.112*\"get\"')\n",
      "(16, '-0.240*\"government\" + 0.238*\"sy\" + -0.194*\"pkk\" + -0.192*\"key\" + -0.170*\"encryption\" + -0.149*\"greek\" + -0.133*\"greece\" + -0.133*\"system\" + -0.130*\"communication\" + -0.119*\"camp\"')\n",
      "(17, '0.387*\"bit\" + 0.255*\"block\" + 0.246*\"people\" + 0.162*\"mode\" + 0.141*\"data\" + 0.132*\"error\" + 0.131*\"sy\" + -0.121*\"pkk\" + 0.119*\"message\" + -0.119*\"year\"')\n",
      "(18, '0.286*\"sy\" + 0.201*\"government\" + -0.185*\"bit\" + -0.180*\"word\" + -0.176*\"armenian\" + 0.174*\"people\" + -0.156*\"god\" + -0.135*\"hai\" + -0.123*\"block\" + 0.120*\"year\"')\n",
      "(19, '-0.252*\"one\" + -0.222*\"bit\" + 0.211*\"would\" + -0.155*\"block\" + 0.138*\"word\" + 0.125*\"work\" + -0.119*\"pkk\" + 0.113*\"hai\" + -0.113*\"year\" + 0.111*\"people\"')\n",
      "(20, '-0.382*\"sy\" + 0.242*\"people\" + -0.202*\"god\" + -0.154*\"would\" + 0.146*\"church\" + 0.146*\"community\" + 0.139*\"word\" + 0.125*\"year\" + 0.119*\"hai\" + 0.117*\"taize\"')\n",
      "(21, '0.311*\"would\" + -0.193*\"government\" + -0.175*\"god\" + -0.149*\"people\" + 0.146*\"work\" + 0.142*\"community\" + 0.134*\"taize\" + 0.120*\"time\" + -0.119*\"game\" + -0.118*\"key\"')\n",
      "(22, '0.227*\"people\" + -0.200*\"key\" + -0.197*\"christian\" + -0.187*\"system\" + -0.156*\"encryption\" + 0.149*\"know\" + 0.129*\"pkk\" + -0.114*\"communication\" + 0.110*\"said\" + -0.107*\"armenian\"')\n",
      "(23, '-0.246*\"christian\" + 0.179*\"comic\" + -0.164*\"like\" + 0.144*\"taize\" + -0.140*\"pkk\" + -0.138*\"car\" + 0.137*\"year\" + 0.117*\"life\" + 0.110*\"government\" + 0.108*\"well\"')\n",
      "(24, '-0.319*\"comic\" + -0.167*\"car\" + 0.116*\"would\" + -0.115*\"war\" + -0.114*\"like\" + -0.114*\"book\" + 0.111*\"sy\" + -0.110*\"new\" + -0.099*\"man\" + 0.099*\"bit\"')\n",
      "(25, '0.372*\"lib\" + 0.355*\"libx11\" + -0.160*\"would\" + 0.139*\"car\" + 0.136*\"sy\" + -0.128*\"comic\" + -0.122*\"one\" + 0.117*\"window\" + -0.103*\"genocide\" + 0.100*\"taize\"')\n",
      "(26, '-0.335*\"car\" + 0.316*\"lib\" + 0.302*\"libx11\" + 0.132*\"christian\" + -0.124*\"like\" + -0.104*\"god\" + -0.095*\"much\" + -0.095*\"back\" + 0.094*\"comic\" + -0.093*\"seat\"')\n",
      "(27, '-0.297*\"lib\" + -0.284*\"libx11\" + 0.265*\"sy\" + -0.223*\"genocide\" + 0.172*\"word\" + -0.143*\"russian\" + 0.135*\"comic\" + 0.120*\"hai\" + 0.119*\"name\" + -0.114*\"armenian\"')\n",
      "(28, '-0.167*\"marriage\" + 0.155*\"document\" + 0.152*\"problem\" + 0.149*\"work\" + -0.142*\"think\" + 0.139*\"christian\" + 0.135*\"system\" + -0.127*\"law\" + -0.121*\"lib\" + -0.119*\"fact\"')\n",
      "(29, '-0.253*\"believe\" + -0.185*\"lib\" + -0.177*\"libx11\" + -0.165*\"one\" + 0.136*\"get\" + 0.136*\"god\" + 0.135*\"game\" + -0.129*\"car\" + -0.124*\"belief\" + 0.118*\"problem\"')\n"
     ]
    }
   ],
   "source": [
    "for item in lsimodel.show_topics():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing with the data from 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Text does not contain num_docs on the first line.\n"
     ]
    }
   ],
   "source": [
    "id2word = gensim.corpora.Dictionary.load_from_text('data/wiki_en_wordids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(newsgroups.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(newsgroups.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "\n",
      "Jesus did and so do I.\n",
      "\n",
      "Peace be with you,\n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups.data))\n",
    "print(newsgroups.data[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-09dc5bcd161d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewsgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-09dc5bcd161d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewsgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'process' is not defined"
     ]
    }
   ],
   "source": [
    "doc_clean = [process(doc).split() for doc in newsgroups.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for simple samples\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704.1523542404175\n"
     ]
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "\n",
    "start = time.time()\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "Lsi = gensim.models.lsimodel.LsiModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "lsimodel = Lsi(doc_term_matrix, num_topics=3, id2word = dictionary)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.005*\"one\" + 0.005*\"would\" + 0.004*\"year\" + 0.004*\"get\" + 0.003*\"time\" + 0.003*\"like\" + 0.003*\"game\" + 0.003*\"well\" + 0.003*\"also\" + 0.003*\"know\"'), (1, '0.009*\"1\" + 0.007*\"x\" + 0.006*\"2\" + 0.006*\"0\" + 0.006*\"file\" + 0.004*\"window\" + 0.004*\"use\" + 0.004*\"system\" + 0.004*\"image\" + 0.004*\"3\"'), (2, '0.007*\"would\" + 0.007*\"one\" + 0.007*\"people\" + 0.006*\"god\" + 0.004*\"think\" + 0.004*\"say\" + 0.004*\"it\" + 0.004*\"know\" + 0.004*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxax\" + 0.003*\"like\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.976*\"x\" + 0.097*\"file\" + 0.079*\"entry\" + 0.052*\"program\" + 0.051*\"0\" + 0.035*\"oname\" + 0.033*\"output\" + 0.030*\"char\" + 0.030*\"line\" + 0.028*\"section\"'), (1, '0.700*\"0\" + 0.470*\"1\" + 0.390*\"2\" + 0.205*\"3\" + 0.155*\"4\" + 0.080*\"5\" + 0.078*\"6\" + -0.069*\"x\" + 0.068*\"7\" + 0.053*\"8\"'), (2, '1.000*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxax\" + 0.008*\"mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v\" + 0.004*\"14\" + 0.004*\"part\" + 0.003*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxaxq\" + 0.002*\"end\" + 0.002*\"m8axaxaxaxaxaxaxaxaxaxaxaxaxax\" + 0.001*\"maxaxaxaxaxaxaxaxaxaxaxaxaxaxasq\" + 0.001*\"maxaxaxaxaxaxaxaxaxaxaxaxaxax1f\" + -0.001*\"0\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lsimodel.print_topics(num_topics=3, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "print(common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
